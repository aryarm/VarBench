import warnings
from pathlib import Path
from snakemake.utils import min_version

##### set minimum snakemake version #####
min_version("6.3.0")

# IMPORT CONFIG VARIABLES
configfile: "config/config.yaml"
# remove any trailing slashes in directories
out = str(Path(config['out']))
data = str(Path(config['data_dir']))
ref = config['ref']
truth = config['develop_in']['truth'] if config['develop'] else config['full_run']['truth']
reads = {}

# if we're in development mode, get the reads from DataHub instead of the data dir
if config['develop']:
    for tech in config['develop_in']['tech']:
        reads[tech] = config['develop_in']['tech'][tech]
else:
    for tech in config['full_run']['tech']:
        reads[tech] = config['full_run']['tech'][tech]

# WHICH CALLERS TO RUN?
callers = {
    'gatk', 'freebayes' , 'sniffles', 'longshot', 'nanosv', 'nanopolish', 'picky',
    'clair3', 'cutesv', 'strelka'
}
callers.remove('nanopolish') # TODO: remove this!
callers.remove('strelka') # TODO: remove this!
if config['callers']:
    # double check that the user isn't asking for callers that we can't run
    if not callers.issuperset(config['callers']):
        warnings.warn("Not all of the callers requested can be executed. Proceeding with as many callers as is possible...")
    callers = callers.intersection(config['callers'])

rule all:
   input:
        # confusion = out+"/matrix.tsv",
        vcfs = expand(out+"/{tech}/variants/{caller}.vcf.gz", tech = reads.keys() , caller = callers)

rule download_data:
    params:
        data_dir = data
    output:
        truth_set = data+"/HG001_GRCh38_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer.vcf.gz",
        illumina = reads['illumina'],
        ont = reads['ont'],
        pacbio = data+"/sorted_final_merged.bam" # TODO: add this to reads dict above
    conda: "envs/default.yaml"
    shell:
        "scripts/download_giab.sh {params.data_dir}"

rule longshot:
    input:
        bam = lambda wildcards: reads[wildcards.tech],
        ref = ref
    output:
        vcf = out+"/{tech}/variants/longshot.vcf.gz",
        orig = out+"/{tech}/variants/longshot/orig.vcf.gz"
    log:
        "logs/longshot_{tech}.log"
    benchmark:
        "bench/longshot_{tech}.tsv"
    conda: "envs/longshot.yaml"
    shell:
        """
        # -A = Automatically calculate mean coverage for region and set max coverage to mean_coverage + 5*sqrt(mean_coverage)
        # -y = Minimum quality (Phred-scaled) of read->haplotype assignment (for read separation). [default: 20.0]
        longshot -A -y 30 --bam {input.bam} --ref {input.ref} --out {output.orig} 2> {log}
        bcftools reheader -f -Oz -o {output.vcf} {input.ref} {output.orig}
        """

rule picky:
    input:
        bam = lambda wildcards: reads[wildcards.tech],
        ref = ref
    output:
        vcf = out+"/{tech}/variants/picky.vcf.gz",
        dir = directory(out+"/{tech}/variants/picky")
    log:
        "logs/picky_{tech}.log"
    benchmark:
        "bench/picky_{tech}.tsv"
    conda: "envs/picky.yaml"
    shell:
        "mkdir -p {output.dir} && "
        "samtools sort -O sam -n {input.bam} | picky.pl sam2align | picky.pl callSV --oprefix {output.dir}/ && "
        "picky.pl xls2vcf --xls {output.dir}/.profile.DEL.xls --xls {output.dir}/.profile.INS.xls "
        "--xls {output.dir}/.profile.INDEL.xls --xls {output.dir}/.profile.INV.xls --xls "
        "{output.dir}/.profile.TTLC.xls --xls {output.dir}/.profile.TDSR.xls --xls "
        "{output.dir}/.profile.TDC.xls | gzip > {output.vcf} 2> {log}"
        
rule prepare_clair3:
    output:
        directory("resources/Clair3/models")
    shell:
        """
        cd resources
        # clone Clair3
        git clone https://github.com/HKU-BAL/Clair3.git
        cd Clair3

        # download pre-trained models
        mkdir models
        wget http://www.bio8.cs.hku.hk/clair3/clair3_models/clair3_models.tar.gz 
        tar -zxvf clair3_models.tar.gz -C ./models
        """

rule clair3:
    input:
        modeldir = rules.prepare_clair3.output,
        bam = lambda wildcards: reads[wildcards.tech],
        ref = ref
    output:
        vcf = out+"/{tech}/variants/clair3.vcf.gz"
    log:
        "logs/clair3_{tech}.log"
    benchmark:
        "bench/clair3_{tech}.tsv"
    params:
        dev_chr = "--ctg_name="+config['develop_in']['chr'] if config['develop'] else "",
        output_prefix = out+"/{tech}/variants",
        platform = lambda wildcards: {"ont": "ont", "pacbio": "hifi", "illumina": "ilmn"}[wildcards.tech]
    conda: "envs/clair3.yaml"
    threads: 2
    shell:
        """
        cd resources/Clair3
        
        ./run_clair3.sh \
        --ref_fn="../../{input.ref}" \
        --bam_fn="../../{input.bam}" \
        --threads={threads} \
        --platform={params.platform} \
        --model_path=`pwd`"/models/{params.platform}" \
        --output="../../{params.output_prefix}" \
        {params.dev_chr}
        """

rule nanosv:
    input:
        bam = lambda wildcards: reads[wildcards.tech],
        bed = config['nanosv_bed']
    output:
        vcf = out+"/{tech}/variants/nanosv.vcf.gz"
    params:
        output_prefix = "NA12878_ont", 
        sample_name = "NA12878_ont"
    log:
        "logs/nanosv_{tech}.log"
    benchmark:
        "bench/nanosv_{tech}.tsv"
    conda: "envs/nanosv.yaml"
    threads: 2
    shell:
        """
        NanoSV -t {threads} -s $(which samtools) -b {input.bed} {input.bam} -o {output.vcf} 2> {log}
        """

rule nanopolish:
    input:
#        fq = lambda wildcards: {something}[wildcards.tech], # need to download fastq files!!
        bam = lambda wildcards: reads[wildcards.tech],
        ref = ref
    output:
        vcf = out+"/{tech}/variants/nanopolish.vcf.gz"
    log:
        "logs/nanopolish_{tech}.log"
    benchmark:
        "bench/nanopolish_{tech}.tsv"
    params:
        data_dir = str(data)
    conda: "envs/nanopolish.yaml"
    threads: 2
    shell:
        """
        nanopolish variants --reads {input.fq} --genome {input.ref} --bam {input.bam} -p 2 --outfile {output.vcf} 2> {log}
        """

rule gatk:
    input:
        bam = lambda wildcards: reads[wildcards.tech],
        ref = ref
    output:
        vcf = out+"/{tech}/variants/gatk.vcf.gz"
    params:
        filter = lambda wildcards: "--read-filter AllowAllReadsReadFilter --disable-tool-default-read-filters" if wildcards.tech == 'ont' else ""
    log:
        "logs/gatk_{tech}.log"
    benchmark:
        "bench/gatk_{tech}.tsv"
    conda: "envs/gatk.yaml"
    shell:
        "gatk HaplotypeCaller {params.filter} -R {input.ref} -I {input.bam} -O {output.vcf}"

rule freebayes:
    input:
        ref=ref,
        samples = lambda wildcards: reads[wildcards.tech],
        # the matching BAI indexes have to present for freebayes
        indexes = lambda wildcards: reads[wildcards.tech]+".bai"
    output:
        vcf = out+"/{tech}/variants/freebayes.vcf.gz"
    log:
        "logs/freebayes_{tech}.log"
    benchmark:
        "bench/freebayes_{tech}.tsv"
    params:
        extra="",         # optional parameters
        chunksize=100000, # reference genome chunk size for parallelization (default: 100000)
        normalize=False,  # flag to use bcftools norm to normalize indels
    threads: 2
    wrapper:
        "0.74.0/bio/freebayes"

rule sniffles:
    input:
        bam = lambda wildcards: reads[wildcards.tech],
    output:
        vcf = out+"/{tech}/variants/sniffles.vcf.gz"
    log:
        "logs/sniffles_{tech}.log"
    benchmark:
        "bench/sniffles_{tech}.tsv"
    conda: "envs/sniffles.yaml"
    shell: 
        "sniffles -m {input.bam} -v {output.vcf} 2> {log} && sed -i 's/STRANDBIAS/PASS/' {output.vcf}"        

rule cutesv:
    input:
        bam = lambda wildcards: reads[wildcards.tech],
        ref = ref
    output:
        vcf = out+"/{tech}/variants/cutesv.vcf.gz"
    log:
        "logs/cutesv_{tech}.log"
    benchmark:
        "bench/cutesv_{tech}.tsv"
    conda: "envs/cutesv.yaml"
    shell: 
        "cuteSV {input.bam} {input.ref} {output.vcf} ./ 2> {log}"
        
rule index_vcf:
    input:
        vcf = out+"/{tech}/variants/{caller}.vcf.gz"
    output:
        vcf = out+"/{tech}/variants/{caller}.bgzp.vcf.gz",
        idx = out+"/{tech}/variants/{caller}.bgzp.vcf.gz.tbi"
    log:
        "logs/index/{caller}_{tech}.log"
    benchmark:
        "bench/index/{caller}_{tech}.tsv"
    conda: "envs/default.yaml"
    shell:
        "gzip -c -q -d -f {input.vcf} | bcftools sort -O v | bgzip > {output.vcf} && "
        "tabix -p vcf {output.vcf} 2> {log}"

rule happy:
    input:
        truth = truth,
        query = rules.index_vcf.output.vcf, 
        query_idx = rules.index_vcf.output.idx,
        genome = ref,
        genome_index = ref+".fai"
    output:
        multiext(out+"/{tech}/happy/{caller}/{caller}",".runinfo.json",".vcf.gz",".summary.csv",
                ".extended.csv",".metrics.json.gz",".roc.all.csv.gz",
                ".roc.Locations.INDEL.csv.gz",".roc.Locations.INDEL.PASS.csv.gz",
                ".roc.Locations.SNP.csv.gz",".roc.tsv")
    params:
        engine="vcfeval",
        prefix=lambda wc, input, output: output[0].split('.')[0],
        ## parameters such as -L to left-align variants
        extra="--verbose",
        qual = "--roc GQX"
    log: "logs/{tech}/happy_{caller}.log"
    threads: 2
    benchmark:
        "bench/{tech}/happy_{caller}.tsv"
    conda: "envs/happy.yaml"
    shell:
        "hap.py --threads {threads} --engine {params.engine} -r {input.genome} {params.extra} -o {params.prefix} --roc GQX --logfile {log} {input.truth} {input.query}"


rule prepare_wittyer:
    output:
        directory("resources/witty.er/Wittyer")
    conda: "envs/dotnet.yaml"
    shell:
        """
        # clone repo
        git clone https://github.com/Illumina/witty.er.git
        
        # compile the repo
        cd  witty.er/Wittyer
        dotnet publish . -c Release
        """
        
rule wittyer:
    input:
        witty_dir = rules.prepare_wittyer.output,
        truth = truth,
        query = rules.index_vcf.output.vcf, 
        query_idx = rules.index_vcf.output.idx,
        config = config['wittyer_config']
    output: 
        dir = directory(out+"/{tech}/wittyer/{caller}")
    conda: "envs/dotnet.yaml"
    shell:
        """
        cd {input.witty_dir}
        
        # run the dll
        dotnet bin/Release/netcoreapp2.0/Wittyer.dll \
            -i {input.query} \
            -t {input.truth} \
            --configFile {input.config} \
            -em cts \
            -o {output.dir}
        """
