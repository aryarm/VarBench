import warnings
from pathlib import Path
from snakemake.utils import min_version

##### set minimum snakemake version #####
min_version("6.3.0")

# IMPORT CONFIG VARIABLES
configfile: "config/config.yaml"
# remove any trailing slashes in directories
out = str(Path(config['out']))
data = str(Path(config['data_dir']))
reads = {
    'ont': data+"/NA12878-minion-ul_GRCh38.bam",
    'illumina': data+"/RMNISTHS_30xdownsample.bam"
}
# if we're in development mode, get the reads from DataHub instead of the data dir
if config['develop']:
    for tech in config['develop_in']['tech']:
        reads[tech] = config['develop_in']['tech'][tech]
    ref = config['develop_in']['annot']['ref']
    strelka_bed = config['develop_in']['annot']['strelka_bed']
    nanosv_bed = config['develop_in']['annot']['nanosv_bed']
        
# WHICH CALLERS TO RUN?
callers = {
    'gatk', 'freebayes', 'strelka', 'sniffles', 'longshot', 'clair', 'nanosv',
    'nanopolish'
}
callers.remove('nanopolish') # TODO: remove this!
callers.remove('gatk') # TODO: remove this!
callers.remove('clair') # TODO: remove this!
if config['callers']:
    # double check that the user isn't asking for callers that we can't run
    if not callers.issuperset(config['callers']):
        warnings.warn("Not all of the callers requested can be executed. Proceeding with as many callers as is possible...")
    callers = callers.intersection(config['callers'])

rule all:
   input:
        # confusion = out+"/matrix.tsv",
        vcfs = expand(out+"/{tech}/variants/{caller}.vcf.gz", tech = ["ont","illumina"] , caller = callers)

rule download_data:
    params:
        data_dir = data
    output:
        truth_set = data+"/HG001_GRCh38_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer.vcf.gz",
        illumina = reads['illumina'],
        ont = reads['ont'],
        pacbio = data+"/sorted_final_merged.bam" # TODO: add this to reads dict above
    conda: "envs/default.yaml"
    shell:
        "scripts/download_giab.sh {params.data_dir}"

rule longshot:
    input:
        bam = lambda wildcards: reads[wildcards.tech],
        ref = ref
    output:
        vcf = out+"/{tech}/variants/longshot.vcf.gz"
    log:
        "logs/longshot_{tech}.log"
    benchmark:
        "bench/longshot_{tech}.tsv"
    conda: "envs/longshot.yaml"
    shell:
        "longshot --bam {input.bam} --ref {input.ref} --out {output.vcf} 2> {log}"

rule clair:
    input:
        bam = lambda wildcards: reads[wildcards.tech],
        ref = ref
    output:
        vcf = out+"/{tech}/variants/clair.vcf.gz"
    log:
        "logs/clair_{tech}.log"
    benchmark:
        "bench/clair_{tech}.tsv"
    params:
        model = "resources/clair_models/ont",
        output_prefix = out+"/{tech}/variants", 
        sample_name = "{tech}"
    conda: "envs/clair.yaml"
    threads: 2
    shell:
        """
        COMMAND=mktemp 
        CLAIR=$(which clair.py)
        
        python $CLAIR callVarBamParallel \
        --chkpnt_fn "{params.model}" \
        --ref_fn "{input.ref}" \
        --bam_fn "{input.bam}" \
        --threshold 0.2 \ # what is this?
        --sampleName "{params.sample_name}" \ 
        --output_prefix "{params.output_prefix}" > $COMMAND 2> {log}
        
        # run Clair with 4 concurrencies
        cat $COMMAND | parallel -j{threads} 2>> {log}
        
        # Find incomplete VCF files and rerun them
        for i in {params.output_prefix}.*.vcf; do if ! [ -z "$(tail -c 1 "$i")" ]; then echo "$i"; fi ; done | grep -f - $COMMAND | sh 2>> {log}
        
        # concatenate vcf files and sort the variants called
        vcfcat {params.output_prefix}.*.vcf | bcftools sort -m 2G | bgziptabix snp_and_indel.vcf.gz 2>> {log}
        """
        
rule nanosv:
    input:
        bam = lambda wildcards: reads[wildcards.tech],
        bed = nanosv_bed
    output:
        vcf = out+"/{tech}/variants/nanosv.vcf.gz"
    params:
        output_prefix = "NA12878_ont", 
        sample_name = "NA12878_ont"
    log:
        "logs/nanosv_{tech}.log"
    benchmark:
        "bench/nanosv_{tech}.tsv"
    conda: "envs/nanosv.yaml"
    threads: 2
    shell:
        """
        NanoSV -t {threads} -s $(which samtools) -b {input.bed} {input.bam} -o {output.vcf} 2> {log}
        """

rule nanopolish:
    input:
#        fq = lambda wildcards: {something}[wildcards.tech], # need to download fastq files!!
        bam = lambda wildcards: reads[wildcards.tech],
        ref = ref
    output:
        vcf = out+"/{tech}/variants/nanopolish.vcf.gz"
    log:
        "logs/nanopolish_{tech}.log"
    benchmark:
        "bench/nanopolish_{tech}.tsv"
    params:
        data_dir = str(data)
    conda: "envs/nanopolish.yaml"
    threads: 2
    shell:
        """
        nanopolish variants --reads {input.fq} --genome {input.ref} --bam {input.bam} -p 2 --outfile {output.vcf} 2> {log}
        """

rule gatk:
    input:
        # single or list of bam files
        bam = lambda wildcards: reads[wildcards.tech],
        ref = ref
    output:
        gvcf = out+"/{tech}/variants/gatk.vcf.gz"
    log:
        "logs/gatk_{tech}.log"
    params:
        extra="",  # optional
        java_opts="-Xmx2G -XX:ParallelGCThreads=2", # 2 threads, 2Gb memory
    benchmark:
        "bench/gatk_{tech}.tsv"
    wrapper:
        "0.74.0/bio/gatk/haplotypecaller"

rule freebayes:
    input:
        ref=ref,
        samples = lambda wildcards: reads[wildcards.tech],
        # the matching BAI indexes have to present for freebayes
        indexes = lambda wildcards: reads[wildcards.tech]+".bai"
    output:
        vcf = out+"/{tech}/variants/freebayes.vcf.gz"
    log:
        "logs/freebayes_{tech}.log"
    benchmark:
        "bench/freebayes_{tech}.tsv"
    params:
        extra="",         # optional parameters
        chunksize=100000, # reference genome chunk size for parallelization (default: 100000)
        normalize=False,  # flag to use bcftools norm to normalize indels
    threads: 2
    wrapper:
        "0.74.0/bio/freebayes"

rule prepare_strelka:
    input:
        # the required bam file
        bam = lambda wildcards: reads[wildcards.tech],
        # path to reference genome fasta and index
        fasta = ref,
        fasta_index = ref+'.fai'
    output:
        # Strelka results - either use directory or complete file path
        dir = directory(out+"/{tech}/variants/strelka")
    log:
        "logs/strelka_{tech}.log"
    benchmark:
        "bench/strelka_{tech}.tsv"
    params:
        # optional parameters
        config_extra="--callRegions "+strelka_bed,
        run_extra=""
    threads: 2
    wrapper:
        "0.74.0/bio/strelka/germline"

rule strelka:
    input:
        dir = out+"/{tech}/variants/strelka"
    params:
        vcf = lambda wildcards, input: input.dir+"/results/variants/genome.S1.vcf"
    output:
        vcf = out+"/{tech}/variants/strelka.vcf.gz"
    conda: "envs/default.yaml"
    shell:
        "ln -s {params.vcf} {output.vcf}"

rule sniffles:
    input:
        bam = lambda wildcards: reads[wildcards.tech],
    output:
        vcf = out+"/{tech}/variants/sniffles.vcf.gz"
    log:
        "logs/sniffles_{tech}.log"
    benchmark:
        "bench/sniffles_{tech}.tsv"
    conda: "envs/sniffles.yaml"
    shell: 
        "sniffles -m {input.bam} -v {output.vcf} 2> {log}"
