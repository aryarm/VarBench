import warnings
from pathlib import Path
from snakemake.utils import min_version

##### set minimum snakemake version #####
min_version("6.3.0")

# IMPORT CONFIG VARIABLES
configfile: "config/config.yaml"
# remove any trailing slashes in directories
out = str(Path(config['out']))
data = str(Path(config['data_dir']))
reads = {
    'ont': data+"/NA12878-minion-ul_GRCh38.bam",
    'illumina': data+"/RMNISTHS_30xdownsample.bam"
}
# if we're in development mode, get the reads from DataHub instead of the data dir
if config['develop']:
    for tech in config['develop_in']:
        reads[tech] = config['develop_in'][tech]

# WHICH CALLERS TO RUN?
callers = {
    'gatk', 'freebayes', 'strelka', 'sniffles', 'longshot', 'clair', 'nanosv',
    'nanopolish'
}
callers.remove('nanopolish') # TODO: remove this!
if config['callers']:
    # double check that the user isn't asking for callers that we can't run
    if not callers.issuperset(config['callers']):
        warnings.warn("Not all of the callers requested can be executed. Proceeding with as many callers as is possible...")
    callers = callers.intersection(config['callers'])

rule all:
   input:
        # confusion = out+"/matrix.tsv",
        vcfs = expand(out+"/{tech}/variants/{caller}.vcf.gz", tech = ["ont","illumina"] , caller = callers)

rule download_data:
    params:
        data_dir = data
    output:
        truth_set = data+"/HG001_GRCh38_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer.vcf.gz",
        illumina = reads['illumina'],
        ont = reads['ont'],
        pacbio = data+"/sorted_final_merged.bam" # TODO: add this to reads dict above
    conda: "envs/default.yaml"
    shell:
        "scripts/download_giab.sh {params.data_dir}"

rule longshot:
    input:
        bam = lambda wildcards: reads[wildcards.tech],
        ref = config['ref']
    output:
        vcf = out+"/{tech}/variants/longshot.vcf.gz"
    log:
        "logs/longshot_{tech}.log"
    benchmark:
        "bench/longshot_{tech}.tsv"
    conda: "envs/longshot.yaml"
    shell:
        "longshot --bam {input.bam} --ref {input.ref} --out {output.vcf} 2> {log}"

rule clair:
    input:
        bam = lambda wildcards: reads[wildcards.tech],
        ref = config['ref']
    output:
        vcf = out+"/{tech}/variants/clair.vcf.gz"
    log:
        "logs/clair_{tech}.log"
    benchmark:
        "bench/clair_{tech}.tsv"
    params:
        model = "resources/clair_models/ont",
        output_prefix = out+"/{tech}/variants", 
        sample_name = "{tech}"
    conda: "envs/clair.yaml"
    threads: 2
    shell:
        """
        COMMAND=mktemp 
        CLAIR=$(which clair.py)
        
        python clair.py callVarBamParallel \
        --chkpnt_fn "{params.model}" \
        --ref_fn "{input.ref}" \
        --bam_fn "{input.bam}" \
        --threshold 0.2 \ # what is this?
        --sampleName "{params.sample_name}" \ 
        --output_prefix "{params.output_prefix}" > $COMMAND 2> {log}
        
        # run Clair with 4 concurrencies
        cat $COMMAND | parallel -j{threads} 2>> {log}
        
        # Find incomplete VCF files and rerun them
        for i in {params.output_prefix}.*.vcf; do if ! [ -z "$(tail -c 1 "$i")" ]; then echo "$i"; fi ; done | grep -f - $COMMAND | sh 2>> {log}
        
        # concatenate vcf files and sort the variants called
        vcfcat {params.output_prefix}.*.vcf | bcftools sort -m 2G | bgziptabix snp_and_indel.vcf.gz 2>> {log}
        """
        
rule nanosv:
    input:
        bam = lambda wildcards: reads[wildcards.tech],
    output:
        vcf = out+"/{tech}/variants/nanosv.vcf.gz"
    params:
        output_prefix = "NA12878_ont", 
        sample_name = "NA12878_ont",
    log:
        "logs/nanosv_{tech}.log"
    benchmark:
        "bench/nanosv_{tech}.tsv"
    conda: "envs/nanosv.yaml"
    threads: 2
    shell:
        """
        NanoSV -t {threads} -s $(which samtools) {input.bam} -o {output.vcf} 2> {log}
        """

rule nanopolish:
    input:
#        fq = lambda wildcards: {something}[wildcards.tech], # need to download fastq files!!
        bam = lambda wildcards: reads[wildcards.tech],
        ref = config['ref']
    output:
        vcf = out+"/{tech}/variants/nanopolish.vcf.gz"
    log:
        "logs/nanopolish_{tech}.log"
    benchmark:
        "bench/nanopolish_{tech}.tsv"
    params:
        data_dir = str(data)
    conda: "envs/nanopolish.yaml"
    threads: 2
    shell:
        """
        nanopolish variants --reads {input.fq} --genome {input.ref} --bam {input.bam} -p 2 --outfile {output.vcf} 2> {log}
        """

rule gatk:
    input:
        # single or list of bam files
        bam = lambda wildcards: reads[wildcards.tech],
        ref = config['ref']
    output:
        vcf = out+"/{tech}/variants/gatk.vcf.gz"
    log:
        "logs/gatk_{tech}.log"
    benchmark:
        "bench/gatk_{tech}.tsv"
    resources:
        mem_mb=1024 #Not sure if we need this .. 
    wrapper:
        "0.74.0/bio/gatk/haplotypecaller"

rule freebayes:
    input:
        ref=config['ref'],
        samples = lambda wildcards: reads[wildcards.tech],
        # the matching BAI indexes have to present for freebayes
        indexes = lambda wildcards: reads[wildcards.tech]+".bai"
    output:
        vcf = out+"/{tech}/variants/freebayes.vcf.gz"
    log:
        "logs/freebayes_{tech}.log"
    benchmark:
        "bench/freebayes_{tech}.tsv"
    params:
        extra="",         # optional parameters
        chunksize=100000, # reference genome chunk size for parallelization (default: 100000)
        normalize=False,  # flag to use bcftools norm to normalize indels
    threads: 2
    wrapper:
        "0.74.0/bio/freebayes"

rule prepare_strelka:
    input:
        # the required bam file
        bam = lambda wildcards: reads[wildcards.tech],
        # path to reference genome fasta and index
        fasta = config['ref'],
        fasta_index = config['ref']+'.fai'
    output:
        # Strelka results - either use directory or complete file path
        dir = directory(out+"/{tech}/variants/strelka")
    log:
        "logs/strelka_{tech}.log"
    benchmark:
        "bench/strelka_{tech}.tsv"
    params:
        # optional parameters
        config_extra="",
        run_extra=""
    threads: 2
    wrapper:
        "0.74.0/bio/strelka/germline"

rule strelka:
    input:
        dir = out+"/{tech}/variants/strelka"
    params:
        vcf = lambda wildcards, input: input.dir+"/results/variants/genome.S1.vcf"
    output:
        vcf = out+"/{tech}/variants/strelka.vcf.gz"
    conda: "envs/default.yaml"
    shell:
        "ln -s {params.vcf} {output.vcf}"

rule sniffles:
    input:
        bam = lambda wildcards: reads[wildcards.tech],
    output:
        vcf = out+"/{tech}/variants/sniffles.vcf.gz"
    log:
        "logs/sniffles_{tech}.log"
    benchmark:
        "bench/sniffles_{tech}.tsv"
    conda: "envs/sniffles.yaml"
    shell: 
        "sniffles -m {input.bam} -v {output.vcf} 2> {log}"

rule happy:
    input:
        truth=config['truth'],
        query = out+"/{tech}/variants/{caller}.vcf.gz"
        genome=config['ref'],
        genome_index=config['ref']+".fai"
    output:
        multiext(out+"/{tech}/happy/{caller}",".runinfo.json",".vcf.gz",".summary.csv",
                ".extended.csv",".metrics.json.gz",".roc.all.csv.gz",
                ".roc.Locations.INDEL.csv.gz",".roc.Locations.INDEL.PASS.csv.gz",
                ".roc.Locations.SNP.csv.gz",".roc.tsv")
    params:
        engine="vcfeval",
        prefix=lambda wc, input, output: output[0].split('.')[0],
        ## parameters such as -L to left-align variants
        extra="--verbose"
    log: "happy.log"
    threads: 2
    wrapper: "0.74.0/bio/hap.py/hap.py"
